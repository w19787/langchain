"""Wrapper around Chatglm2 APIs."""
from __future__ import annotations

import logging
import zhipuai

from typing import (
    Any,
    Dict,
    List,
    Optional,
)

import requests

from langchain.callbacks.manager import (
    CallbackManagerForLLMRun,
)
from langchain.llms.base import LLM
from langchain.pydantic_v1 import BaseModel, Extra, Field, PrivateAttr, root_validator
from langchain.utils import get_from_dict_or_env

logger = logging.getLogger(__name__)


class _Chatglm2EndpointClient(BaseModel):
    """An API client that talks to a Chatglm2 llm endpoint."""

    api_key: str
    model: str
    temperature: float
    top_p: float
    incremental: bool  


    def post(self, request: Any) -> Any:
        zhipuai.api_key = self.api_key

        logger.info("model_name: {0}, temperature: {1}, top_p: {2}, incremental: {3}", 
                     self.model, self.temperature, self.top_p, self.incremental)

        response = zhipuai.model_api.sse_invoke(
            model=self.model,
            prompt=request,
            temperature=self.temperature,
            top_p=self.top_p,
            incremental=self.incremental
        )

        data = ""
        for event in response.events():
            data += event.data

        return data


class Chatglm2(LLM):
    """Wrapper around Chatglm2 large language models.
    To use, you should have the environment variable
    ``Chatglm2_API_KEY`` and ``Chatglm2_GROUP_ID`` set with your API key,
    or pass them as a named parameter to the constructor.
    Example:
     .. code-block:: python
         from langchain.llms.Chatglm2 import Chatglm2
         Chatglm2 = Chatglm2(model="<model_name>", Chatglm2_api_key="my-api-key",
          Chatglm2_group_id="my-group-id")
    """

    _client: _Chatglm2EndpointClient = PrivateAttr()
    model: str = "chatglm_lite"
    """Model name to use."""
    temperature: float = 0.2
    """A non-negative float that tunes the degree of randomness in generation."""
    top_p: float = 0.8
    """Total probability mass of tokens to consider at each step."""
    incremental: bool = True
    model_kwargs: Dict[str, Any] = Field(default_factory=dict)
    """Holds any model parameters valid for `create` call not explicitly specified."""

    Chatglm2_api_key: Optional[str] = None

    class Config:
        """Configuration for this pydantic object."""

        extra = Extra.forbid

    @root_validator()
    def validate_environment(cls, values: Dict) -> Dict:
        """Validate that api key and python package exists in environment."""
        values["Chatglm2_api_key"] = get_from_dict_or_env(
            values, "Chatglm2_api_key", "CHATGLM2_API_KEY"
        )

        return values

    @property
    def _default_params(self) -> Dict[str, Any]:
        """Get the default parameters for calling OpenAI API."""
        return {
            "model": self.model,
            "temperature": self.temperature,
            "top_p": self.top_p,
            "incremental": self.incremental,
            **self.model_kwargs,
        }

    @property
    def _identifying_params(self) -> Dict[str, Any]:
        """Get the identifying parameters."""
        return {**{"model": self.model}, **self._default_params}

    @property
    def _llm_type(self) -> str:
        """Return type of llm."""
        return "Chatglm2"

    def __init__(self, **data: Any):
        super().__init__(**data)
        self._client = _Chatglm2EndpointClient(
            api_key=self.Chatglm2_api_key,
            model = self.model,
            temperature = self.temperature,
            top_p = self.top_p,
            incremental = self.incremental

        )

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        r"""Call out to Chatglm2's completion endpoint to chat
        Args:
            prompt: The prompt to pass into the model.
        Returns:
            The string generated by the model.
        Example:
            .. code-block:: python
                response = Chatglm2("Tell me a joke.")
        """
        request = self._default_params
        request["messages"] = [{"role": "user", "content": prompt}]
        request.update(kwargs)
        response = self._client.post(request)

        return response
